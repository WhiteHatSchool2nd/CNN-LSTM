{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 불러오기\n",
    "여러 라이브러리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e8d221f190>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "rcParams['figure.figsize'] = 14, 10\n",
    "register_matplotlib_converters()\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_from_csv(target):\n",
    "    return pd.read_csv(target).rename(columns=lambda x: x.strip())\n",
    "\n",
    "def dataframe_from_csvs(targets):\n",
    "    return pd.concat([dataframe_from_csv(x) for x in targets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATASET = sorted([x for x in Path(\"./test/\").glob(\"*.csv\")])\n",
    "TRAIN_DATASET = sorted([x for x in Path(\"./train/\").glob(\"*.csv\")])\n",
    "TEST_DF_RAW = dataframe_from_csvs(TEST_DATASET)\n",
    "TRAIN_DF_RAW = dataframe_from_csvs(TRAIN_DATASET)\n",
    "ATTACK_DF = TEST_DF_RAW['attack']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유효필드 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_FIELD = [\"time\", \"attack_P1\", \"attack_P2\", \"attack_P3\",\"attack\"]\n",
    "VALID_COLUMNS_IN_TRAIN_DATASET = TRAIN_DF_RAW.columns.drop(DROP_FIELD) # DROP_FIELD를 통해 normalization에 사용하지 않을 변수를 제거함.\n",
    "TAG_MIN = TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET].min()\n",
    "TAG_MAX = TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df, TAG_MIN, TAG_MAX):\n",
    "    ndf = df.copy()\n",
    "    for c in df.columns:\n",
    "        if TAG_MIN[c] == TAG_MAX[c]:\n",
    "            ndf[c] = df[c] - TAG_MIN[c]\n",
    "        else:\n",
    "            ndf[c] = (df[c] - TAG_MIN[c]) / (TAG_MAX[c] - TAG_MIN[c])\n",
    "    return ndf\n",
    "\n",
    "# Min-Max Normalize\n",
    "TRAIN_DF = normalize(TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET], TAG_MIN, TAG_MAX).ewm(alpha=0.9).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규화 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, False, False)\n"
     ]
    }
   ],
   "source": [
    "def boundary_check(df):\n",
    "    x = np.array(df, dtype=np.float32)\n",
    "    return np.any(x > 1.0), np.any(x < 0), np.any(np.isnan(x))\n",
    "\n",
    "# Boundary Check\n",
    "print(boundary_check(TRAIN_DF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 슬라이딩 윈도우 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비지도 학습을 위한 데이터셋 구성\n",
    "window_size = 60\n",
    "label_size = 30000\n",
    "def sliding_window_unsupervised(df, window_size, feature_columns, answer_column):\n",
    "    data = df[feature_columns].values\n",
    "    answers = answer_column.values\n",
    "\n",
    "    num_samples = len(df) - window_size\n",
    "    features = np.empty((num_samples, window_size, len(feature_columns)), dtype=np.float32)\n",
    "    answer_targets = np.empty(num_samples, dtype=int)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        features[i] = data[i:i+window_size]\n",
    "        answer_targets[i] = 1 if np.any(answers[i:i+window_size] == 1) else 0\n",
    "\n",
    "    return features, answer_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특정 컬럼 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29940, 60, 30)\n",
      "(29940,)\n"
     ]
    }
   ],
   "source": [
    "feature_columns = ['P1_B2004', 'P1_B2016', 'P1_B3004', 'P1_B3005', 'P1_B4002', 'P1_B4005', 'P1_B400B',\n",
    "                   'P1_B4022', 'P1_FCV01D', 'P1_FCV01Z', 'P1_FCV02D', 'P1_FCV02Z', 'P1_FCV03D',\n",
    "                   'P1_FCV03Z', 'P1_FT01', 'P1_FT01Z', 'P1_FT02', 'P1_FT02Z', 'P1_FT03', 'P1_FT03Z',\n",
    "                   'P1_LCV01D', 'P1_LIT01', 'P1_PCV01D', 'P1_PCV01Z', 'P1_PCV02D', 'P1_PCV02Z',\n",
    "                   'P1_PIT01', 'P1_PIT02', 'P1_TIT01', 'P1_TIT02']\n",
    "\n",
    "features, answers = sliding_window_unsupervised(TRAIN_DF[:label_size], 60, feature_columns, ATTACK_DF[:label_size])\n",
    "print(features.shape)\n",
    "print(answers.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분할\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, answers, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy 배열을 torch Tensor로 변환\n",
    "features_train = torch.tensor(features_train, dtype=torch.float32)\n",
    "features_test = torch.tensor(features_test, dtype=torch.float32)\n",
    "labels_train = torch.tensor(labels_train, dtype=torch.float32)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AnomalyDetector(nn.Module):\n",
    "    # 레이어 초기화 생성자\n",
    "    def __init__(self, n_features, n_hidden, seq_len, n_layers):\n",
    "        super(AnomalyDetector, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.seq_len = seq_len\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # CNN 레이어\n",
    "        self.c1 = nn.Conv1d(in_channels=n_features, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # LSTM 레이어\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64,\n",
    "            hidden_size=n_hidden,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # 선형 레이어\n",
    "        self.linear = nn.Linear(in_features=n_hidden, out_features=1)\n",
    "\n",
    "    # 히든 상태 초기화\n",
    "    def reset_hidden_state(self, batch_size):\n",
    "        self.hidden = (\n",
    "            torch.zeros(self.n_layers, batch_size, self.n_hidden).to(next(self.parameters()).device),\n",
    "            torch.zeros(self.n_layers, batch_size, self.n_hidden).to(next(self.parameters()).device)\n",
    "        )\n",
    "\n",
    "    # 순전파 메서드\n",
    "    def forward(self, sequences):\n",
    "        # (batch_size, seq_len, n_features) -> (batch_size, n_features, seq_len)\n",
    "        sequences = sequences.permute(0, 2, 1)\n",
    "\n",
    "        # CNN 적용\n",
    "        sequences = self.c1(sequences)\n",
    "        sequences = self.relu(sequences)\n",
    "\n",
    "        # (batch_size, n_features, seq_len) -> (batch_size, seq_len, n_features)\n",
    "        sequences = sequences.permute(0, 2, 1)\n",
    "\n",
    "        # LSTM 적용\n",
    "        lstm_out, self.hidden = self.lstm(sequences, self.hidden)\n",
    "\n",
    "        # 마지막 타임스텝 출력\n",
    "        last_time_step = lstm_out[:, -1, :]\n",
    "\n",
    "        # 선형 레이어 적용\n",
    "        y_pred = self.linear(last_time_step)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def train_model(model, train_data, train_labels, val_data=None, val_labels=None, num_epochs=100, verbose=10, patience=20):\n",
    "    model.to(device)\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "    train_hist = []\n",
    "    val_hist = []\n",
    "    f1_hist = []\n",
    "    for t in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        for idx, seq in enumerate(train_data):\n",
    "            model.reset_hidden_state(batch_size=1)\n",
    "            seq = torch.unsqueeze(seq, 0).to(device)\n",
    "            y_pred = model(seq)\n",
    "            loss = loss_fn(y_pred[0], train_labels[idx].unsqueeze(0).to(device))\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            epoch_loss += loss.item()\n",
    "        train_hist.append(epoch_loss / len(train_data))\n",
    "        if val_data is not None:\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                model.eval()\n",
    "                predictions = []\n",
    "                for val_idx, val_seq in enumerate(val_data):\n",
    "                    model.reset_hidden_state(batch_size=1)\n",
    "                    val_seq = torch.unsqueeze(val_seq, 0).to(device)\n",
    "                    y_val_pred = model(val_seq)\n",
    "                    val_step_loss = loss_fn(y_val_pred[0], val_labels[val_idx].unsqueeze(0).to(device))\n",
    "                    val_loss += val_step_loss.item()\n",
    "                    predictions.append(y_val_pred[0].item())\n",
    "            val_hist.append(val_loss / len(val_data))\n",
    "            predictions = (np.array(predictions) > 0.5).astype(int)\n",
    "            f1 = f1_score(val_labels.cpu().numpy(), predictions)\n",
    "            f1_hist.append(f1)\n",
    "            if t % verbose == 0:\n",
    "                print(f'Epoch {t} train loss: {epoch_loss / len(train_data)} val loss: {val_loss / len(val_data)} F1 score: {f1:.2f}')\n",
    "            if (t % patience == 0) & (t != 0):\n",
    "                if val_hist[t - patience] < val_hist[t]:\n",
    "                    print('\\n Early Stopping')\n",
    "                    break\n",
    "        elif t % verbose == 0:\n",
    "            print(f'Epoch {t} train loss: {epoch_loss / len(train_data)}')\n",
    "    return model, train_hist, val_hist, f1_hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 초기화 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 0.2302138715952575 val loss: 0.2066825633556268 F1 score: 0.00\n",
      "Epoch 1 train loss: 0.19968160146656996 val loss: 0.18891543116104467 F1 score: 0.00\n",
      "Epoch 2 train loss: 0.18786303882419306 val loss: 0.17848948177271234 F1 score: 0.00\n",
      "Epoch 3 train loss: 0.18199419955497156 val loss: 0.17400689594293325 F1 score: 0.00\n",
      "Epoch 4 train loss: 0.17813856395816277 val loss: 0.17082122846711215 F1 score: 0.00\n",
      "Epoch 5 train loss: 0.17536116174298402 val loss: 0.16845578144372267 F1 score: 0.00\n",
      "Epoch 6 train loss: 0.17310897827163368 val loss: 0.1663303856072859 F1 score: 0.00\n",
      "Epoch 7 train loss: 0.1709434811633053 val loss: 0.16505839618429313 F1 score: 0.00\n",
      "Epoch 8 train loss: 0.16898440151072855 val loss: 0.16230196360954063 F1 score: 0.00\n",
      "Epoch 9 train loss: 0.16656861249071284 val loss: 0.16102615930513295 F1 score: 0.00\n",
      "Epoch 10 train loss: 0.16470754911666444 val loss: 0.15846925390586905 F1 score: 0.00\n",
      "Epoch 11 train loss: 0.1625974107304751 val loss: 0.15701661361462288 F1 score: 0.00\n",
      "Epoch 12 train loss: 0.16053585821617103 val loss: 0.15335608911299276 F1 score: 0.00\n",
      "Epoch 13 train loss: 0.15851748973360122 val loss: 0.15104736486274398 F1 score: 0.00\n",
      "Epoch 14 train loss: 0.15619179083354168 val loss: 0.14899994738016753 F1 score: 0.00\n",
      "Epoch 15 train loss: 0.15427671985560126 val loss: 0.14713783926002647 F1 score: 0.00\n",
      "Epoch 16 train loss: 0.152423990437476 val loss: 0.14390343483400567 F1 score: 0.00\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화 및 학습\n",
    "model = AnomalyDetector(\n",
    "    n_features=features_train.shape[2],\n",
    "    n_hidden=4,\n",
    "    seq_len=features_train.shape[1],\n",
    "    n_layers=1\n",
    ")\n",
    "\n",
    "\n",
    "model, train_hist, val_hist, accuracy_hist = train_model(\n",
    "    model,\n",
    "    features_train,\n",
    "    labels_train,\n",
    "    features_test,\n",
    "    labels_test,\n",
    "    num_epochs=100,\n",
    "    verbose=1,\n",
    "    patience=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "torch.save(model.state_dict(), 'anomaly_detector_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측 및 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 및 검증 손실 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_hist, label='Train Loss')\n",
    "plt.plot(val_hist, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# F1 스코어 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(f1_hist, label='Validation F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 예측 및 성능 평가\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = []\n",
    "    for seq in features_test:\n",
    "        seq = torch.unsqueeze(seq, 0).to(device)\n",
    "        y_pred = model(seq)\n",
    "        predictions.append(y_pred[0].item())\n",
    "predictions = np.array(predictions)\n",
    "predictions = (predictions > 0.5).astype(int)  # 0.5를 기준으로 이진 분류\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "# F1 스코어 계산\n",
    "f1 = f1_score(labels_test, predictions)\n",
    "print(f'Final F1 Score: {f1:.2f}')\n",
    "\n",
    "# 혼동 행렬 그리기\n",
    "cm = confusion_matrix(labels_test, predictions, labels=[0, 1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
